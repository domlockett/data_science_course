min_date = min(tweets$date, na.rm = TRUE)
max_date = max(tweets$date, na.rm = TRUE)
# earliest date: "2014-01-01"
# latest date: "2020-02-14"
# subset into retweets
tweets = tweets %>%
filter(!is_retweet)
# 5 most popular tweets
tweets_pop = tweets %>%
arrange(desc(favorite_count))
most_pop_tweets = tweets_pop[1:5, "text"]
# 5 most popular:
# [1] "A$AP Rocky released from prison and on his way home to the United States from Sweden. It was a Rocky Week get home ASAP A$AP!"
# [2] "https://t.co/VXeKiVzpTf"
# [3] "All is well! Missiles launched from Iran at two military bases located in Iraq. Assessment of casualties &amp; damages taking place now. So far so good! We have the most powerful and well equipped military anywhere in the world by far! I will be making a statement tomorrow morning."
# [4] "MERRY CHRISTMAS!"
# [5] "Kobe Bryant despite being one of the truly great basketball players of all time was just getting started in life. He loved his family so much and had such strong passion for the future. The loss of his beautiful daughter Gianna makes this moment even more devastating...."
# 5 most retweeted tweets
tweets_retweet = tweets %>%
arrange(desc(retweet_count))
most_retweeted_tweets = tweets_retweet[1:5, "text"]
# 5 most retweeted:
# [1] "#FraudNewsCNN #FNN https://t.co/WYUnHjjUjg"
# [2] "TODAY WE MAKE AMERICA GREAT AGAIN!"
# [3] "Why would Kim Jong-un insult me by calling me \"old\" when I would NEVER call him \"short and fat?\" Oh well I try so hard to be his friend - and maybe someday that will happen!"
# [4] "A$AP Rocky released from prison and on his way home to the United States from Sweden. It was a Rocky Week get home ASAP A$AP!"
# [5] "Such a beautiful and important evening! The forgotten man and woman will never be forgotten again. We will all come together as never before"
# cleaning tweets
tweets$text = gsub("\\s+"," ",tweets$text)
tweets$text = gsub("\\d", "", tweets$text)
tweets$text = gsub('[[:punct:]]+','', tweets$text)
tweets$text = tolower(tweets$text)
stopwords = c("see","people","new","want","one","even","must","need","done","back","just","going", "know",
"can","said","like","many","like","realdonaldtrump")
stopwords_std = stopwords(kind = "SMART")
stopwords = c(stopwords, stopwords_std)
# tweets$text = tweets$text[!(tweets$text %in% stopwords)]
toRemove <- paste0("\\b(", paste0(stopwords, collapse="|"), ")\\b")
tweets$text = gsub(toRemove,'', tweets$text)
# creating data
library(tidytext)
tweets_words <-  tweets %>%
select(text) %>%
unnest_tokens(word, text)
words <- tweets_words %>% count(word, sort=TRUE)
# creating word cloud
set.seed(1234) # for reproducibility
wc = wordcloud(words = words$word, freq = words$n, min.freq = 3,
max.words=200, random.order=FALSE,
rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
# creating document term matrix
text = tweets$text
docs = Corpus(VectorSource(text))
docs <- docs %>%
tm_map(removeNumbers) %>%
tm_map(removePunctuation) %>%
tm_map(stripWhitespace)
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeWords, stopwords("english"))
dtm <- TermDocumentMatrix(docs, control =
list(weighting = weightTfIdf))
matrix <- as.matrix(dtm)
words <- sort(rowSums(matrix),decreasing=TRUE)
df <- data.frame(word = names(words),freq=words)
top_words = df$word[1:50]
tweets = read.csv('https://politicaldatascience.com/PDS/Datasets/trump_tweets.csv')
# split date and time
tweets = tweets %>%
mutate(created_at = str_split(created_at, " ")) %>%
mutate(date = unlist(str_split(tweets$created_at, " "))[(1:nrow(tweets)%%2 != 0)]) %>%
mutate(time = unlist(str_split(tweets$created_at, " "))[(1:nrow(tweets)%%2 == 0)])
# date range
min_date = min(tweets$date, na.rm = TRUE)
max_date = max(tweets$date, na.rm = TRUE)
# subset into retweets
tweets = tweets %>%
filter(!is_retweet)
# 5 most popular tweets
tweets_pop = tweets %>%
arrange(desc(favorite_count))
most_pop_tweets = tweets_pop[1:5, "text"]
# 5 most popular:
# [1] "A$AP Rocky released from prison and on his way home to the United States from Sweden. It was a Rocky Week get home ASAP A$AP!"
# [2] "https://t.co/VXeKiVzpTf"
# [3] "All is well! Missiles launched from Iran at two military bases located in Iraq. Assessment of casualties &amp; damages taking place now. So far so good! We have the most powerful and well equipped military anywhere in the world by far! I will be making a statement tomorrow morning."
# [4] "MERRY CHRISTMAS!"
# [5] "Kobe Bryant despite being one of the truly great basketball players of all time was just getting started in life. He loved his family so much and had such strong passion for the future. The loss of his beautiful daughter Gianna makes this moment even more devastating...."
# 5 most retweeted tweets
tweets_retweet = tweets %>%
arrange(desc(retweet_count))
most_retweeted_tweets = tweets_retweet[1:5, "text"]
# cleaning tweets
tweets$text = gsub("\\s+"," ",tweets$text)
tweets$text = gsub("\\d", "", tweets$text)
tweets$text = gsub('[[:punct:]]+','', tweets$text)
tweets$text = tolower(tweets$text)
stopwords = c("see","people","new","want","one","even","must","need","done","back","just","going", "know",
"can","said","like","many","like","realdonaldtrump")
stopwords_std = stopwords(kind = "SMART")
stopwords = c(stopwords, stopwords_std)
# tweets$text = tweets$text[!(tweets$text %in% stopwords)]
toRemove <- paste0("\\b(", paste0(stopwords, collapse="|"), ")\\b")
tweets$text = gsub(toRemove,'', tweets$text)
# creating data
library(tidytext)
tweets_words <-  tweets %>%
select(text) %>%
unnest_tokens(word, text)
words <- tweets_words %>% count(word, sort=TRUE)
# creating word cloud
set.seed(1234) # for reproducibility
wc = wordcloud(words = words$word, freq = words$n, min.freq = 3,
max.words=200, random.order=FALSE,
rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
# creating document term matrix
text = tweets$text
docs = Corpus(VectorSource(text))
docs <- docs %>%
tm_map(removeNumbers) %>%
tm_map(removePunctuation) %>%
tm_map(stripWhitespace)
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeWords, stopwords("english"))
dtm <- TermDocumentMatrix(docs, control =
list(weighting = weightTfIdf))
matrix <- as.matrix(dtm)
words <- sort(rowSums(matrix),decreasing=TRUE)
df <- data.frame(word = names(words),freq=words)
top_words = df$word[1:50]
# [22] "tonight"               "foxnews"               "obama"
# [25] "job"                   "dont"                  """
# [28] "win"                   "enjoy"                 "show"
# [31] "american"              "day"                   "jobs"
# [34] "years"                 "man"                   "nice"
# [37] "work"                  "border"                "media"
# [40] "watch"                 "amazing"               "cnn"
# [43] "fake"                  "happy"                 "bad"
# [46] "deal"                  "world"                 "poll"
# [49] "clinton"               "night"
>
# get tibble
primaryPolls<-rea_csv('https://jmontgomery.github.io/PDS/Datasets/president_primary_polls_feb2020.csv')
# get tibble
primaryPolls<-read_csv('https://jmontgomery.github.io/PDS/Datasets/president_primary_polls_feb2020.csv')
(primaryPolls$start_date, "%m/%d/%Y")
primaryPolls<-primaryPolls[primaryPolls$candidate_name%in%c("Amy Klobuchar", "Bernard Sanders", "Elizabeth Warren", "Joseph R. Biden Jr.", "Michael Bloomberg", "Pete Buttigieg"),]
# get data from dsired states
poll.data<-primaryPolls[primaryPolls$state %in% c("Alabama", "Arkansas", "California", "Colorado", "Maine", "Massachusetts", "Minnesota", "North Carolina", "Oklahoma", "Tennessee", "Texas", "Utah", "Vermont", "Virginia"),]
ggplot(data=poll.data, mapping=aes(x=start_date, y=pct))+
geom_point(mapping=aes(color=state))+
geom_smooth(se=F)+
facet_wrap(~candidate_name, nrow=3)+
theme_minimal()+
labs(x="Date", y="Percentage of Vote", title="State of Primary Race", color="State")+
theme(axis.text.x=element_text(angle=90, hjust=1)) # rotate x labels
### PART 2
library(tidyverse)
### PART 2
library(tidyverse)
primaryPolls<-read_csv('https://jmontgomery.github.io/PDS/Datasets/president_primary_polls_feb2020.csv')
primaryPolls$start_date<-as.Date(primaryPolls$start_date, "%m/%d/%y")
# get useful columns of data
basicPolls<-select(primaryPolls, state, candidate_name, start_date, pct, sample_size)
# get relevant candidates
basicPolls <- basicPolls %>% filter(candidate_name %in% c("Amy Klobuchar", "Bernard Sanders", "Elizabeth Warren", "Joseph R. Biden Jr.", "Michael Bloomberg", "Pete Buttigieg"))
basicPolls <- basicPolls %>% arrange(state)
# create new column that indicates how many votes the candidate received at each poll. There may be many polls in one state
basicPolls <- basicPolls %>% mutate(votesReceivedAtPoll = round(sample_size * (pct/100)))
basicPolls
# group the data by state and candidate and sum up the total number of votes received in each group.
# data tells us how many votes each candidate got in the entire state
by_state_candidate <- basicPolls %>% group_by(state, candidate_name)
by_state_candidate <- by_state_candidate %>% summarise(votesForCandidateInState=sum(votesReceivedAtPoll))
# need to find total number of votes that were cast in each state
votes_by_state <- basicPolls %>% group_by(state) %>% summarise(totalVotesInTheState=sum(votesReceivedAtPoll))
# for each column store the total number of votes in the state in order to find the percentage of the state that voted for each candidate
by_state_candidate <- by_state_candidate %>% inner_join(votes_by_state, totalVotesInTheState, by='state')
by_state_candidate <- by_state_candidate %>% mutate(percentageOfStateForCandidate = votesForCandidateInState/totalVotesInTheState)
# final dataset
by_state_candidate
print(paste('Original size:', object.size(primaryPolls), 'bytes. New size:', object.size(by_state_candidate), "bytes."))
View(by_state_candidate)
# group the data by state and candidate and sum up the total number of votes received in each group.
# data tells us how many votes each candidate got in the entire state
by_state_candidate <- basicPolls %>% group_by(state, candidate_name)
by_state_candidate <- by_state_candidate %>% summarise(votesForCandidateInState=sum(votesReceivedAtPoll))
# need to find total number of votes that were cast in each state
votes_by_state <- basicPolls %>% group_by(state) %>% summarise(totalVotesInTheState=sum(votesReceivedAtPoll))
# for each column store the total number of votes in the state in order to find the percentage of the state that voted for each candidate
by_state_candidate <- by_state_candidate %>% inner_join(votes_by_state, totalVotesInTheState, by='state')
by_state_candidate <- by_state_candidate %>% mutate(percentageOfStateForCandidate = votesForCandidateInState/totalVotesInTheState)
# final dataset
by_state_candidate
print(paste('Original size:', object.size(primaryPolls), 'bytes. New size:', object.size(by_state_candidate), "bytes."))
### PART 3
library(fivethirtyeight)
library(tidyverse)
polls <- read_csv('https://jmontgomery.github.io/PDS/Datasets/president_primary_polls_feb2020.csv')
Endorsements <- endorsements_2020
Endorsements
Endorsements <- rename(Endorsements, candidate_name=endorsee)
# convert endorsements to tibble
Endorsements<-as_tibble(Endorsements)
# filter polls to only contain candidates with certain names. THen subset down to certain columns
polls$candidate_name
polls <- polls %>%
filter(candidate_name %in% c("Amy Klobuchar", "Bernard Sanders", "Elizabeth Warren", "Joseph R. Biden Jr.", "Michael Bloomberg", "Pete Buttigieg")) %>%
select(candidate_name, sample_size, start_date, party, pct)
unique(Endorsements %>% select(candidate_name))
unique(polls %>% select(candidate_name))
"Joe Biden"))
polls
Endorsements
combined_data <- Endorsements %>% inner_join(polls, by='candidate_name')
unique(combined_data %>% select(candidate_name))
endorsements_by_candidate <- Endorsements %>% group_by(candidate_name) %>% summarise(number_of_endorsements = n()) %>% filter(!is.na(candidate_name))
endorsements_by_candidate
p<-ggplot(data=endorsements_by_candidate, mapping=aes(x=candidate_name, y=number_of_endorsements))+
geom_point()
p+theme_dark()+theme(axis.text.x=element_text(angle=90, hjust=1)) # rotate x labels
p+theme_minimal()+labs(x='Candidate', y='Number of Endorsements', title='Endorsements by Candidate')+theme(axis.text.x=element_text(angle=90, hjust=1)) # rotate x labels
### PART 4
library(tidyverse)
#install.packages('tm')
library(tm)
#install.packages('lubridate')
library(lubridate)
#install.packages('wordcloud')
library(wordcloud)
tweets <- read_csv('https://politicaldatascience.com/PDS/Datasets/trump_tweets.csv')
created_at_time<-unlist(str_split(tweets$created_at, pattern=" "))[c(F,T)]
created_at_time<-str_pad(created_at_time, 5, pad="0")
tweets <- tweets %>% mutate(created_at_date = as.Date(unlist(str_split(created_at, pattern=" "))[c(T,F)], "%m/%d/%Y"), created_at_time = created_at_time)
# sort by date then by time within date. Can sort by date after using as.Date because dates are written as YYYY-MM-DD which will order by date if using lexicographical ordering
# in order to sort time using lexicographical ordering, we need to pad the hours with 0 so that the format is HH:MM, which we do above
# get the first and last elements of the tibble after they are sorted by date to get the range of dates
date_range<-(tweets %>% arrange(created_at_date, created_at_time) %>% select(created_at))[c(1,nrow(tweets)),]
date_range[1,]
print(paste('the tweets are from', date_range[1,], 'to', date_range[2,]))
original_tweets <- tweets %>% filter(is_retweet == FALSE)
# most popular tweets
most_liked <- (original_tweets %>% arrange(-favorite_count))[1:5,]
most_liked$text
# most retweeted tweets
most_retweeted <- (original_tweets %>% arrange(-retweet_count))[1:5,]
most_retweeted
# remove extra whitespace
# description: removes stop words from a list of string
# input: list of strings
# output: list of strings
get_non_stop<-function(tweet){
list_of_stops<-c(stopwords('en'), c('see', 'people','new','want','one','even','must','need','done','back','just','going', 'know', 'can','said','like','many','like','realdonaldtrump'))
a<-tweet[!(tweet %in% list_of_stops)]
return(a)
}
processed_tweets<-original_tweets %>%
select(text) %>%
mutate(text=removePunctuation(text)) %>%
mutate(text=str_remove_all(text, "'|'|"|"|-|amp|RT")) %>%
mutate(text=removeNumbers(text)) %>%
mutate(text=tolower(text)) %>%
mutate(text=removeWords(text, removePunctuation(c(stopwords('en'), c('see', 'people','new','want','one','even','must','need','done','back','just','going', 'know', 'can','said','like','many','like','realdonaldtrump'))))) %>%
mutate(text=stripWhitespace(text)) %>%
mutate(text=str_trim(text))
# use str_split to get all the words to be separated and ulist to get them all in one list
# then use a tibble to count up the number of occurences of each word
counts<-as_tibble(unlist(str_split(processed_tweets$text, pattern=" "))) %>%
group_by(value) %>%
summarise(count = n()) %>%
arrange(-count)
counts
wordcloud(words=counts$value, freq=counts$count, min.freq=3, max.words=50)
# create DTM
corpus<-Corpus(VectorSource(processed_tweets$text))
matrixthing<-TermDocumentMatrix(corpus, control=list(weighting = weightTfIdf))
# get 50 words with highets tf-idf
library('slam')
sort(row_sums(matrixthing), decreasing=T)[1:50]
created_at_time<-unlist(str_split(tweets$created_at, pattern=" "))[c(F,T)]
created_at_time<-str_pad(created_at_time, 5, pad="0")
tweets <- tweets %>% mutate(created_at_date = as.Date(unlist(str_split(created_at, pattern=" "))[c(T,F)], "%m/%d/%Y"), created_at_time = created_at_time)
# sort by date then by time within date. Can sort by date after using as.Date because dates are written as YYYY-MM-DD which will order by date if using lexicographical ordering
# in order to sort time using lexicographical ordering, we need to pad the hours with 0 so that the format is HH:MM, which we do above
# get the first and last elements of the tibble after they are sorted by date to get the range of dates
date_range<-(tweets %>% arrange(created_at_date, created_at_time) %>% select(created_at))[c(1,nrow(tweets)),]
date_range[1,]
print(paste('the tweets are from', date_range[1,], 'to', date_range[2,]))
original_tweets <- tweets %>% filter(is_retweet == FALSE)
# most popular tweets
most_liked <- (original_tweets %>% arrange(-favorite_count))[1:5,]
most_liked$text
# most retweeted tweets
most_retweeted <- (original_tweets %>% arrange(-retweet_count))[1:5,]
most_retweeted
# description: removes stop words from a list of string
# input: list of strings
# output: list of strings
get_non_stop<-function(tweet){
list_of_stops<-c(stopwords('en'), c('see', 'people','new','want','one','even','must','need','done','back','just','going', 'know', 'can','said','like','many','like','realdonaldtrump'))
a<-tweet[!(tweet %in% list_of_stops)]
return(a)
}
processed_tweets<-original_tweets %>%
select(text) %>%
mutate(text=removePunctuation(text)) %>%
mutate(text=str_remove_all(text, "'|'|"|"|-|amp|RT")) %>%
mutate(text=removeNumbers(text)) %>%
mutate(text=tolower(text)) %>%
mutate(text=removeWords(text, removePunctuation(c(stopwords('en'), c('see', 'people','new','want','one','even','must','need','done','back','just','going', 'know', 'can','said','like','many','like','realdonaldtrump'))))) %>%
mutate(text=stripWhitespace(text)) %>%
mutate(text=str_trim(text))
# use str_split to get all the words to be separated and ulist to get them all in one list
# then use a tibble to count up the number of occurences of each word
counts<-as_tibble(unlist(str_split(processed_tweets$text, pattern=" "))) %>%
group_by(value) %>%
summarise(count = n()) %>%
arrange(-count)
counts
wordcloud(words=counts$value, freq=counts$count, min.freq=3, max.words=50)
counts<-as_tibble(unlist(str_split(processed_tweets$text, pattern=" "))) %>%
group_by(value) %>%
summarise(count = n()) %>%
arrange(-count)
counts
counts<-as_tibble(unlist(str_split(processed_tweets$text, pattern=" "))) %>%
group_by(value) %>%
summarise(count = n()) %>%
arrange(-count)
processed_tweets<-original_tweets %>%
select(text) %>%
mutate(text=removePunctuation(text)) %>%
mutate(text=str_remove_all(text, "'|'|"|"|-|amp|RT")) %>%
mutate(text=removeNumbers(text)) %>%
mutate(text=tolower(text)) %>%
mutate(text=removeWords(text, removePunctuation(c(stopwords('en'), c('see', 'people','new','want','one','even','must','need','done','back','just','going', 'know', 'can','said','like','many','like','realdonaldtrump'))))) %>%
mutate(text=stripWhitespace(text)) %>%
mutate(text=str_trim(text))
processed_tweets<-original_tweets %>%
select(text) %>%
mutate(text=removePunctuation(text)) %>%
#  mutate(text=str_remove_all(text, "'|'|"|"|-|amp|RT")) %>%
mutate(text=removeNumbers(text)) %>%
mutate(text=tolower(text)) %>%
mutate(text=removeWords(text, removePunctuation(c(stopwords('en'), c('see', 'people','new','want','one','even','must','need','done','back','just','going', 'know', 'can','said','like','many','like','realdonaldtrump'))))) %>%
mutate(text=stripWhitespace(text)) %>%
mutate(text=str_trim(text))
# use str_split to get all the words to be separated and ulist to get them all in one list
# then use a tibble to count up the number of occurences of each word
counts<-as_tibble(unlist(str_split(processed_tweets$text, pattern=" "))) %>%
group_by(value) %>%
summarise(count = n()) %>%
arrange(-count)
counts
wordcloud(words=counts$value, freq=counts$count, min.freq=3, max.words=50)
# create DTM
corpus<-Corpus(VectorSource(processed_tweets$text))
matrixthing<-TermDocumentMatrix(corpus, control=list(weighting = weightTfIdf))
# get 50 words with highets tf-idf
library('slam')
sort(row_sums(matrixthing), decreasing=T)[1:50]
##Here, I am loading in data, only choosing Super Tuesday states and relevant candidates.
primaryPolls<-read_csv('https://jmontgomery.github.io/PDS/Datasets/president_primary_polls_feb2020.csv')
primaryPolls$start_date<-as.Date(primaryPolls$start_date, "%m/%d/%Y")
primaryPolls<-primaryPolls[primaryPolls$state%in%c("Alabama", "Arkansas", "California", "Colorado", "Maine", "Massachusetts", "Minnesota", "North Carolina", "Oklahoma", "Tennessee", "Texas", "Utah", "Vermont", "Virginia"),]
primaryPolls<-primaryPolls[primaryPolls$candidate_name%in%c("Amy Klobuchar", "Bernard Sanders", "Elizabeth Warren", "Joseph R. Biden Jr.", "Michael Bloomberg", "Pete Buttigieg", "Tom Steyer"),]
#I make a preliminary plot that shows the total for candidates across all of these states.
#Notice that Bloomberg's percent is in the negative and has a huge margin of error. This is
#because there isn't data for Bloomberg. R has to interpolate.
ggplot(data=primaryPolls)+
geom_smooth(mapping = aes(x=start_date, y=pct, color=candidate_name))
#We can interpret this plot by noting that Bernie Sanders and Joe Biden are the top-polling candidates,
#with Bernie on an upswing and Biden on a decline. Yet, this plot is too general. We can separate
#by state. The data separated into the 12 Super Tuesday states are below:
ggplot(data=primaryPolls)+
geom_point(mapping = aes(x=start_date, y=pct, color=candidate_name), alpha=.9) +
facet_wrap(~ state, nrow=2)
#by state
ggplot(data=primaryPolls) +
ggtitle("Candidate Support by State") +
geom_point(mapping = aes(x=start_date, y=pct, color=candidate_name), alpha=.9) +
labs(x="Polling Date", y="Percentage of Support", color = "Candidates")+
facet_wrap(~ state, nrow=2)+
theme_minimal()+
theme(plot.title = element_text(hjust = 0.5), axis.text.x=element_text(angle=90, hjust=1))
#by candidate
ggplot(data=primaryPolls)+
ggtitle("Support for Candidates") +
geom_smooth(mapping = aes(x=start_date, y=pct, color=candidate_name)) +
geom_point(mapping = aes(x=start_date, y=pct, color=candidate_name), alpha=.4) +
facet_wrap(~ candidate_name, nrow=2)+
theme_minimal() +
labs(x="Polling Date", y="Percentage of Support", color = "Candidates") +
theme(plot.title = element_text(hjust = 0.5), axis.text.x=element_text(angle=90, hjust=1))
#2- finish what we started on 2/13/2020
library(dplyr)
library(tidyr)
#only relevant candidates
relevant <- primaryPolls %>%
filter(candidate_name %in% c("Amy Klobuchar", "Bernard Sanders", "Elizabeth Warren", "Joseph R. Biden Jr.", "Michael Bloomberg", "Pete Buttigieg", "Tom Steyer")) %>%
select(candidate_name, pct, state)
dim(relevant) #567x3
#reorganizing
relevant <- relevant %>%
pivot_wider(names_from = state, values_from = pct)
dim(relevant)
#comparing sizes
object.size(primaryPolls)
object.size(relevant)
#reading in data
library(fivethirtyeight)
library(tidyverse)
library(readr)
polls <- read_csv('https://jmontgomery.github.io/PDS/Datasets/president_primary_polls_feb2020.csv')
Endorsements <- endorsements_2020
#changing endorsements variable name endorsee to candidate_name
Endorsements <- rename(Endorsements, candidate_name = endorsee)
colnames(Endorsements)
#changing endorsements data frame into a tibble object
Endorsements <- as_tibble(Endorsements)
class(Endorsements)
#filtering poll to only have the 6 Klobuchar, Sanders, Warren, Biden, Bloomberg, Buttigieg
#and to only candidate_name, sample_size, start_date, party and pct
subpolls <- polls %>%
filter(candidate_name %in% c("Amy Klobuchar", "Bernard Sanders", "Elizabeth Warren", "Joseph R. Biden Jr.", "Michael Bloomberg", "Pete Buttigieg")) %>%
select(candidate_name, sample_size, start_date, party, pct)
subpolls
#find different spellings in the datasets and make them the same
#with only DPLYR
#we can see through setdiff that in subpolls, the names are Bernard Sanders and Joseph R. Biden Jr.
#we can change these to Bernie Sanders and Joe Biden
setdiff(subpolls$candidate_name, Endorsements$candidate_name)
subpolls <- subpolls %>%
mutate(candidate_name = recode(candidate_name, "Bernard Sanders" = "Bernie Sanders", "Joseph R. Biden Jr." = "Joe Biden"))
#checking that I have five candidates
intersect(subpolls$candidate_name, Endorsements$candidate_name)
#combining the two datasets by candidate name using dplyr
combined <- subpolls %>%
inner_join(Endorsements, by = "candidate_name")
unique(combined$candidate_name)
colnames(combined)
#creating a variable which indicates number of endorsements for
#each of the five candidates
countEndorse <- combined %>%
group_by(candidate_name) %>%
add_tally() %>%
filter(!is.na(candidate_name))
colnames(countEndorse)
#plot the number of endorsements each of the 5 candidates have
#save as p
p <- ggplot(data=countEndorse, mapping=aes(x=candidate_name))+
geom_bar()
#run p + theme_dark()
p + theme_dark()
#change x and y labels, adding title, picking a theme
p <- ggplot(data=countEndorse, mapping=aes(x=candidate_name, fill=candidate_name))+
geom_bar()
p + theme_classic() +
ggtitle("Endorsements of Democratic Candidates for POTUS")+
labs(x="Democratic Candidates", y="Number of Endorsements") +
theme(legend.position="none", plot.title = element_text(hjust = 0.5), axis.text.x=element_text(angle=0, hjust=0.5))
library(tidyverse)
library(tm)
library(lubridate)
library(wordcloud)
tweets <- read_csv('https://politicaldatascience.com/PDS/Datasets/trump_tweets.csv')
#separating the created_at variable
as_tibble(tweets)
tweets <- tweets %>%
separate(created_at, c("Date", "Time"), sep = " ")
#report ranges of dates in dataset
dateRange <- range(tweets$Date)
dateRange
#removing retweets
tweets <- tweets %>%
filter(is_retweet == FALSE)
#finding most favorited tweets
tweets <- tweets %>%
arrange(desc(favorite_count))
tweets$text[1:5]
#finding most retweeted tweets
tweets <- tweets %>%
arrange(desc(retweet_count))
tweets$text[1:5]
#Remove extraneous whitespace and making all lowercase
words <- str_split(tweets$text, pattern = " ")
words <- str_c(unlist(words))
words <- str_to_lower(words)
#removing punctuation and special characters
words <- str_replace_all(words, "[^[:alnum:]]", "")
#removing all numbers
words <- removeNumbers(words)
#removing stop words and blank spaces
fillers <- c(stopwords("en"), "see", "im", "people", "new", "want", "one", "even", "must", "need", "done", "back", "just", "going", "know", "can", "said", "like", "realdonaldtrump", "will", "rt", "")
content <- words[!(words %in% fillers)]
length(content)
#making a word cloud requires frequencies of terms
frequencies <- as.tibble(content) %>%
group_by(word = value) %>%
summarise(count = n()) %>%
arrange(desc(count))
#making the actual word cloud
wordcloud(frequencies$word, frequencies$count, min.freq = 3, max.words = 50)
#Create a document term matrix called DTM that includes the argument
#control = list(weighting = weightTfIdf)
content <- VectorSource(tweets$text)
contentCorpus <- SimpleCorpus(content, control = list(language = "en"))
DTM <- DocumentTermMatrix(contentCorpus, control = list(removeNumbers = T, stopWords = T, weighting = weightTfIdf))
dim(DTM)
#reporting 50 words with highest tf.idf scores using a lower frequency bound of .8
head(findFreqTerms(DTM, lowfreq = 0.8), n=50)
